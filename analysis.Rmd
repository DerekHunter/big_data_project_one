---
title: "Project_01"
author: "Group_1"
date: "March 21, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = TRUE)
```

```{r include = FALSE, echo = FALSE}
library(glmnet)
library(randomForest)
library(dplyr)
```
The main problem with missing values in this dataset, is that a missing value means something. Specifically, for instance, most of the predictors relating to basement, an NA/missing value encodes the value of a lack of basement. The same is true for predictors relating to garage. So, what we chose to do is for all the factors that have a missing value, we replace them with a None level for that factor. i.e. for basement quality, an NA means no basement, so for basement quality we replace those with a factor level of None. For the non-factor predictors, we just replaced them with 0. This method is very general, and it may be a little mis-representative of what is happening, but we felt this was an okay trade-off. This was all the cleaning that our group chose to do to the data.

```{r include = FALSE, echo = FALSE, warning = FALSE}
train = read.csv("./train.csv")
test = read.csv("./test.csv")
test = mutate(test, SalePrice = rep(0, 1459))
houses = bind_rows(train, test)

    cleanData = function(data){
      tmpData = subset(data, select = -Id)
      
      tmpData$MSSubClass = as.factor(tmpData$MSSubClass);#Factor encoded as a numeric value
      for(column in colnames(tmpData)){
        if(class(tmpData[[column]]) == "factor"){
          tmpData[column] = factor(tmpData[[column]], levels=c("None", levels(tmpData[[column]])))
          tmpData[[column]][is.na(tmpData[[column]])] = "None"
        }else if(class(tmpData[[column]]) == "character"){
          tmpData[column] = factor(tmpData[[column]], levels=c("None", levels(tmpData[[column]])))
          tmpData[[column]][is.na(tmpData[[column]])] = "None"
        }else{
          tmpData[column][is.na(tmpData[column])] = 0;
        }
      }
      return (tmpData)
    }

houses = cleanData(houses)
train = filter(houses, SalePrice > 0)
test = filter(houses, SalePrice == 0)

# sum(is.na(train))
# sum(is.na(test))
# sum(sapply(train[, 1:79], class) != sapply(test[, 1:79], class))
```

```{r, fig.height=6, cache=TRUE}
rf1 = randomForest(x = train[, -80], y = train$SalePrice, importance = TRUE, data = train)

varImpPlot(rf1)

testpredrf1 = predict(rf1, test)
testpredrf1 = cbind(1461:2919, testpredrf1)
colnames(testpredrf1) = c("Id", "SalePrice")
write.csv(testpredrf1, file = "testpredrf1.csv", row.names = FALSE)
```

The predictor variables that showed to have some effect and for which we plan to use for a linear regression model are Overall Quality, Neighbohood, External Quality, above ground square feet, and size of garage in car capacity. Most of these factors make a lot of sense as to why the sales price would be affected. Better quality, better neighborhood, and larger square feet intuitively should increase the sales price. However, the size of the garage does seem a bit suprizing considering all the other factors that were measured.  

###Random Forests Tuning
From:
James, Gareth, et al. An introduction to statistical learning. Vol. 6. New York: springer, 2013.

"When building these decision trees, each time a split in a tree is considered, a random sample of
m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose m ≈√p"

So for tuning our Random Forests we will use the specification m= root(p) as a base and then use the tuneRF function to find the optimal number of splits. Variables with only one factor are removed to improve accuracy.  

### RF2

```{r}
m_data<-floor(sqrt(ncol(train[, -80])))
set.seed(10)
x<- data.frame(train[,c(-80)])



drops<- c("Utilities", "Condition2", "RoofMatl","HouseStyle", "Exterior1st", "Exterior2nd", "Electrical", "Heating", "GarageQual", "PoolQC", "MiscFeature")

x2<-x[,!(names(x)%in% drops)]

y<- train$SalePrice

tuneRF(x2, y, stepFactor=1.15, improve= .001, ntree=500, mtryStart=26)
tuneRF(x2, y, stepFactor=1.15, improve= .001, ntree=1000, mtryStart=26)

```

We've found that the best m for this analysis is 29 for 500 and 1000 trees. It performs roughly 2% better than the default analysis. This is probably not enough to make a large difference, but the model will be slightly more aaccurate.

```{r, fig.height=6, cache=TRUE}
set.seed(10)
rf2 = randomForest(x = x2, y = y, importance = TRUE, mtry = 29)
rf2
randomForest(x2 , y)
varImpPlot(rf2)

testpredrf2 = predict(rf2, test)
testpredrf2 = cbind(1461:2919, testpredrf2)
colnames(testpredrf2) = c("Id", "SalePrice")
write.csv(testpredrf2, file = "testpredrf2.csv", row.names = FALSE)
```

The tuned configuration is ultimately not making a large difference in the results . 


```{r}
mse = function(linmod) sum(linmod$residuals^2)/linmod$df.residual
```
Created a function to calculate the MSE for our linear models.

```{r include = FALSE, echo = FALSE}
# # Linear model using all of the variables
# lm1 = lm(SalePrice ~ ., data = train)
# anova(lm1)
# mse(lm1)
# sqrt(mse(lm1))
# 
# # 10-fold crossvalidation 
# Sales.xval = rep(0, nrow(train))
# xvs = rep(1:10, length = nrow(train))
# xvs = sample(xvs)
# for (i in 1:10) {
#   xvstest = train[xvs == i,]
#   xvstrain = train[xvs != i,]
#   glub = lm(SalePrice ~ ., data = xvstrain)
#   Sales.xval[xvs == i] = predict(glub, xvstest)
#   if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
# }

# testpred1 = predict(lm1, test) # There are no MSSubClass 190 in the training data, but there are 31 in the testing data. The linear model doesn't know how to handle these observations.
# length(testpred1)
# length(1461:2919)
# testpred1 = cbind(1461:2919, testpred1)
# colnames(testpred1) = c("Id", "SalePrice")
# write.csv(testpred1, file = "testpred1.csv", row.names = FALSE)
```

```{r, fig.height=6}
# Linear model using important variables from RF selection IncNodePurity
lm2 = lm(SalePrice ~ OverallQual + Neighborhood + GrLivArea + ExterQual + GarageCars, data = train)
anova(lm2)
mse(lm2)
sqrt(mse(lm2))
summary(lm2)[8]
summary(lm2)[9]

Sales.xval = rep(0, nrow(train))
xvs = rep(1:10, length = nrow(train))
xvs = sample(xvs)
for (i in 1:10) {
  xvstest = train[xvs == i,]
  xvstrain = train[xvs != i,]
  glub = lm(SalePrice ~ OverallQual + Neighborhood + GrLivArea + ExterQual + GarageCars, data = xvstrain)
  Sales.xval[xvs == i] = predict(glub, xvstest)
  if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
}

testpred2 = predict(lm2, test)
length(testpred2)
length(1461:2919)
testpred2 = cbind(1461:2919, testpred2)
colnames(testpred2) = c("Id", "SalePrice")
# write.csv(testpred2, file = "testpred2.csv", row.names = FALSE)

par(mfrow = c(2, 2))
plot(lm2)
```

```{r include = FALSE, echo = FALSE}
# # Linear model using important variables from RF selection %IncMSE
# lm3 = lm(SalePrice ~ GrLivArea + Neighborhood + OverallQual + TotalBsmtSF + MSSubClass, data = train)
# anova(lm3)
# mse(lm3)
# sqrt(mse(lm3))
# 
# Sales.xval = rep(0, nrow(train))
# xvs = rep(1:10, length = nrow(train))
# xvs = sample(xvs)
# for (i in 1:10) {
#   xvstest = train[xvs == i,]
#   xvstrain = train[xvs != i,]
#   glub = lm(SalePrice ~ GrLivArea + Neighborhood + OverallQual + TotalBsmtSF + MSSubClass, data = xvstrain)
#   Sales.xval[xvs == i] = predict(glub, xvstest)
#   if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
# }
# 
# testpred3 = predict(lm3, test) # There are no MSSubClass 190 in the training data, but there are 31 in the testing data. The linear model doesn't know how to handle these observations.
# summary(train$MSSubClass)
# summary(test$MSSubClass)
```

After performing a Random Forest variable selection, the IncNodePurity variable importance plot showed the top five variables of OverallQual, Neighborhood, GrLivArea, ExterQual, GarageCars. These make sense intuitively as good predictors for the sale price of a home as stated above. We ran a linear model, called lm2, and calculated the root MSE as 34843.58. Although this linear model violates the assumptions of normality and nonconstant variance as seen in the diagnostic plots, we accept it as a baseline for futher models. We submitted an entry to Kaggle, and ranked 1769 out of 2055. `r round(1769/2055, 2)*100`% of entries have a lower root mean squared logged error. Additionally, the linear model's $R^2$ = 0.812, and the adjusted $R^2$ = 0.808.

```{r}
full<-lm(train$SalePrice~ ., data = train)
coef.back<-step(full, direction = "backward", trace = FALSE, steps=5000)
coef.for<-step(full, direction = "forward", trace = FALSE, steps=5000)
coef.both <- step(full, direction="both", trace = FALSE, steps=5000)
```

```{r}
#turnoff scientific notation 
options(scipen=999)
summary(coef.back)
length(coefficients(coef.back))
```

These can be difficult to see given all the factor variables. 

1.MSSubClass
2. MSZoning
3. Lot Frontage
4. Lot Area
5. Street Pave
6. Land Contour 
7. Utilities
8. Lot Config
9. Land Slope
10. Neighborhood
11. Condition
12. Overall Quality
13. Overall Condition
14. Year Built
15. Year Remodeled
16. Roof Style 
17. Roof Material
18. Exterior Type
19. MasVnrArea
20. External Condition
21. Foundation
22. Basement Quality
23. Basement Condition
24. Basement Exposure
25. Basement Finish
26. Heating Type
27. Central Air
28. 1st Flr Sq Ft
29. 2nd Flr Sq Ft
30. Low QUality Finish
31. Basement FUll Bath
32. Full Bath Number
33. Half Bath Number
34. Kitchen Above Ground
35. Kitchen Quality
36. TOtal Rooms Above Ground 
37. Functional
38. FirePlaces
39. Garage Yr Blt
40. Garage Area
41. Garage Condition
42. Garage Quality
43. WoodDeck SF
44. Enclosed Porch
45 3 Season Porch
46. Screen Porch
47. Pool Area
48. Pool Quality
49. Fence Type
50. Sale Type
51. Sale Condition
52. HasOpenPorch 

So backwards elimination eliminated roughly 27 variables 

Has a R Square of .9425, Adjusted R Square of .9337
Has 193 Terms (Including Factors)

```{r}
summary(coef.for)
```
### Unique Variables in the Set From Forward Elimination
1. Alley Type
2. Lot Shape
3. Building Type
4. House Style
5. 2nd Exterior Type
6. Exterior Quality
7. Basement FInished SF
8. Basement FInished SF 2
9. Basement Unfinished SF
10. Electrical
11. Basement Half Bath
12. Bedroom Above Ground
13. Fireplace Quality
14. Garage Type
15. Garage Finish
16. Garage Cars
17. Paved Drive
18. Open Porch Sfs
19. MIsc Feature
20. MoSold
21. Yr Sold
22. Has2ndFlr
23.HasWoodDeck
24.HasEnclosedPorch
25.Has3SsnPorch

Has an R square of .9459, Adjusted R square .9334
273 Terms

```{r}
summary(coef.both)
```
This evaluation returned the same results as backwards selection. 

So we see that forward elimination evaluation explains roughly .2% more of the variation in the data, than backwards selection, but it has roughly 80 more terms (including factors), which equates to the evaluation being roughly 40% bigger for .2% increase in accuracy. 

The largest effects in the backwards elimination model are 
1. MS Subclass 160
2. MS Zoning (all terms)
3. Lot Area
4. Land Slope Severe 
5. StoneBridge Neighborhood
6. Normal Condition
7. Condition2PosN
8. Overall Quality
9. Overall Conidtion
10. Year Built 
11. Year Remodeled
12. Roof material (Huge Effect)
13. Basement Finished SF 1 & 2
14. Basment Unifinished SF
15. Heating QCTA
16. Central Air
17. 1st Floor SF 
18. 2nd Floor SF
19. Significant Effect for Kitchen Quality Below Excellent
20. Functional MAJ 2
21. 1 Fireplace
22. Garage Area
23. Wood Deck SF
24. Screen Porch
25. Sale Condition Normal 


It appears that all the values we selected for in the Random Forests Analysis show up in the stepwise regression, however they show up in a much smaller scale. 
