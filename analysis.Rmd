---
title: "Project_01"
author: "Group_1"
date: "March 21, 2017"
output: pdf_document
---

```{r}
getwd()
library(glmnet)
library(randomForest)
```

```{r include = FALSE, echo = FALSE}
    loadData = function(path){
      tmpData = read.csv(path)
      tmpData = subset(tmpData, select = -Id)
      
      tmpData$MSSubClass = as.factor(tmpData$MSSubClass);#Factor encoded as a numeric value
      for(column in colnames(tmpData)){
        if(class(tmpData[[column]]) == "factor"){
          tmpData[column] = factor(tmpData[[column]], levels=c("None", levels(tmpData[[column]])))
          tmpData[[column]][is.na(tmpData[[column]])] = "None"
        }else{
          tmpData[column][is.na(tmpData[column])] = 0;
        }
      }
      return (tmpData)
    }


train = loadData("./train.csv")
test = loadData("./test.csv")

sum(is.na(train))
sum(is.na(test))
    
levels(train$MSSubClass) = c("20",  "30",  "40",  "45",  "50",  "60",  "70",  "75",  "80",  "85",  "90",  "120", "150", "160", "180", "190")
    
```

Random Forest
```{r}
rf1 = randomForest(x = train[, 1:79], y = train$SalePrice, importance = TRUE, data = train)

varImpPlot(rf1)
```

```{r}
mse = function(linmod) sum(linmod$residuals^2)/linmod$df.residual

# Linear model using all of the variables
lm1 = lm(SalePrice ~ ., data = train)
anova(lm1)
mse(lm1)

Sales.xval = rep(0, nrow(train))
xvs = rep(1:10, length = nrow(train))
xvs = sample(xvs)
for (i in 1:10) {
  xvstest = train[xvs == i,]
  xvstrain = train[xvs != i,]
  glub = lm(SalePrice ~ ., data = xvstrain)
  Sales.xval[xvs == i] = predict(glub, xvstest)
  if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
}

testpred1 = predict(lm1, train)


# Linear model using important variables from RF selection
lm2 = lm(SalePrice ~ OverallQual + Neighborhood + GrLivArea + ExterQual + GarageCars, data = train)
anova(lm2)
mse(lm2)

Sales.xval = rep(0, nrow(train))
xvs = rep(1:10, length = nrow(train))
xvs = sample(xvs)
for (i in 1:10) {
  xvstest = train[xvs == i,]
  xvstrain = train[xvs != i,]
  glub = lm(SalePrice ~ OverallQual + Neighborhood + GrLivArea + ExterQual + GarageCars, data = xvstrain)
  Sales.xval[xvs == i] = predict(glub, xvstest)
  if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
}

testpred2 = predict(lm2, test)
testpred2 = cbind(1461:2919, testpred2)
colnames(testpred2) = c("Id", "SalePrice")
write.csv(testpred2, file = "testpred2.csv", row.names = FALSE)


# Linear model using important variables from RF selection
lm3 = lm(SalePrice ~ GrLivArea + Neighborhood + OverallQual + TotalBsmtSF + MSSubClass, data = train)
anova(lm3)
mse(lm3)

Sales.xval = rep(0, nrow(train))
xvs = rep(1:10, length = nrow(train))
xvs = sample(xvs)
for (i in 1:10) {
  xvstest = train[xvs == i,]
  xvstrain = train[xvs != i,]
  glub = lm(SalePrice ~ GrLivArea + Neighborhood + OverallQual + TotalBsmtSF + MSSubClass, data = xvstrain)
  Sales.xval[xvs == i] = predict(glub, xvstest)
  if (i == 10) print(sum((train$SalePrice - Sales.xval)^2)/glub$df.residual)
}

testpred3 = predict(lm3, test) # There are no MSSubClass 190 in the training data, but there are 31 in the testing data. The linear model doesn't know how to handle these observations. ???
summary(train$MSSubClass)
summary(test$MSSubClass)

# So far, it only looks like lm2 is working entirely.
```

